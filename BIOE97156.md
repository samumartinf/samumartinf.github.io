---
layout: single
title: An interactive simulation for BIOE97156
author_profile: true
permalink: /robot/
---

This is meant to be an axiliary short explanation and discussion about target interception for Imperial College's module on Animal Locomotion and Bioinspired Robotics (ALBiR).

## The Simulation
Like in periodism, I will take the inverted pyramid approach for this writing. This report will start with the simulation and my current implementation of the robot, hopefully this will showing some results 
<div id="sketch-holder"></div>

## Problem Specification
During the spring term of 2020 in ALBiR we have studied the different ways locomotion and environment perception and interaction works in the animal kingdom. From biped motion, to visual recognition and targer pursuit, we have explored Nature's solution for these problem. ~~Copying Nature~~ Inspired by Nature we were expected to take what we had learnt and use it to the real world. A small robot was built using two brush motors, a Raspberry Pi and a Pixy2 camera attached to a servo. 


We are tasked with 

## Real-life Implementation - Sensing the environment
For the real-life assessment we were expected to catch a red robot

### Distance estimation
One of the most important pieces of the algorithm is a precise distance estimation function. Without it critical information such as angles with respect to obstacles and target would be impossible to acquire. Our only input for estimating said distance is the camera feed, therefore, we are determining the distance by the angular declination below the horizon. As it can be seen from the figure below (Ooi et al. 2001), we can find the distance to the target (d) by measuring its angle with respect to the horizon. 

![Distance estimation](/assets/images/robotSimulation/horizontalDistance.jpg){:class="img-responsive"}

For the robot, we know the camera to be tilted by 20 degrees, which will be our initial angle (alpha). To that alpha, we would like to add another angle (beta) that will be the infered angle within the robot's field of view as seen below.

![Distance calculation](/assets/images/robotSimulation/distanceCalculation.png){:class="img-responsive"}

This calculations allow us to find distance d, that will aid us with the rest of the inference. 

### Angle estimation
Similarly to the distance estimation, the angle estimation is infered by the pixel distance to the centre of the feed and the camera's FoV. 

![Distance calculation](/assets/images/robotSimulation/angleCalculation.png){:class="img-responsive"}
The final angle will be gamma + servoPosition, the servo position is returned in angles already by the servo library in Python, therefore it is very easy to find. 

### The controllers
The robot requires two different controllers to work properly. The first controller manages the "head" (the servo motor attached to the camera), whereas the second controller is in charge of the robot's body position by setting the turning rate of the wheels to steer left or right. 

The different controllers require different behaviour, the head controller needs to be both smooth and precise, as any rapid movement of the robot (especially when close) should be tracked properly as we do not want the prey to leave the robot's FoV. Leaving the FoV would mean that we lose the ability to estimate both distance and angle, which would significantly handicap our robot's ability to catch the prey. Therefore the head controller PID(0.04, 0, 0.08) has a higher pGain and dGain and can even allow for some overshoot if necessary to keep the prey in the FoV (it is set slightly underdamped).

The wheel's controller needs to be smoother and it should not make abrupt changes PID(0.005, 0, 0.04) so it is set to be overdamped. This overdamped wheel controller also helps in the obstacle avoidance, when the obstacle leaves the FoV, the robot has no way of knowing where the obstacle is. Therefore, this overdamped behaviour allows for smooth turning around the obstacle without turning into it even if the prey is close. (This behaviour is especially noticeable in the simulation and videos, so the reader is encouraged to inspect those)

### Making predictions
As stated before, the robot will be able to extract information about angle and distance only if the prey is in the FoV of the camera, but what happens when the prey is obstructed by an obstacle or the prey makes a sharp turn? In this instance we would like to make predictions based on our previously available information about the prey and obstacles. While the prey is seen, the robot will not only chase it, but also gather information about the prey's and obstacle's angle, distance, and how these change over time.  These allow us to make short-term predictions. When the prey is not in the FoV, a different script triggers that will use this previous information to guess both the prey and obstacle's angle and distance in the next update. The script will make an average of the change in angle and size in the last 20 frames and use that to update obstacles' and prey's position and angle. 
$$
angle(t+1) = angle(t) + average(\partial angle)
$$
After this, the new predicted angle will be appended to the vector storing this information and the cycle can start again. However, this creates a problem as the predicted angle could grow infinitely. Therefore, a decaying factor for the dAngle is put in place so that it will decrease as time continues. This behaviour can be observed in the videos underneath.

<iframe width="560" height="315" src="https://www.youtube.com/embed/y9JwQ56jD2o" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<iframe width="560" height="315" src="https://www.youtube.com/embed/APKulRcC2Zc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Without a prediction, the robot will stop when it stops seeing the prey, wasting time, one of the performance indicators for the target tracking algorithms. 



### Avoiding the obstacles

Obstacle avoidance is another of the main objectives of the coursework. The principle used in our robot is rather simple, we want the robot to follow the prey, but if the obstacle is within a certain threshold, we want to modify our trajectory to avoid bumping into them without disrupting our time to catch. To avoid turning unnecessarily, the threshold depends on both the distance to the obstacle and the angle at which the obstacle is with respect to the robot. If the angle is greater than 30 degrees, it is  unlikely that the robot will bump against the obstacle regardless of the proximity and if the target is further than a set threshold (d = 0.5m) the obstacle will be ignored. However, if both conditions are met, we want to ensure that the new trajectory is both smooth and the shortest path to the prey. Here we decide whether we want to avoid the obstacle from the left or the right. One possibility would be to steer to the left if the object is to the right and vice versa. However, we are not taking into account the prey's velocity in that decision,  in the situation below, which path should the robot take?

![Distance calculation](/assets/images/robotSimulation/pathChoice.png){:class="img-responsive"}

The robot implements a small check, that will take into account the prey's velocity and it will choose the path (left or right) accordingly. Finally, once the path is chosen the robot also takes into account the obstacle's width and measures the angle not from the centre but the obstacle's side (left or right accordingly) to make the turns as sharp as possible. In the video underneath, the robot can be seen turning right to avoid the obstacle even though the prey is on the left, showing the algorithm in place.



<iframe width="560" height="315" src="https://www.youtube.com/embed/qktChqYHf8I" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>



## Real-life Implementation - The flowchart
I don't yet have this, but I will message you as soon as I have it Julie. 

<script src="https://cdn.jsdelivr.net/npm/p5@1.0.0/lib/p5.js"></script>
<script src="https://raw.githubusercontent.com/processing/p5.js/1.0.0/src/dom/dom.js"></script>
<script src="/assets/js/p5library/p5.clickable.js"></script>
<script src="/assets/js/p5library/robotSimulation.js"></script>